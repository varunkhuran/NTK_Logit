{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do:\n",
    "\n",
    "1. Given neural net, form the NTK and do gradient descent on the kernel regression.\n",
    "2. Consolidate the resampling of the data, the forming of the neural net and NTK, and training into same class.\n",
    "3. Perform the test that we have multiple times (~100 times) to get a smoothed version of the permutation test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Given two datasets $A$ and $B$, which come respectively from distributions $P$ and $Q$, two-sample tests are basically a statistical test that tries to answer the hypothesis $H_0: P = Q$ (or reject it in favor of $H_1: P \\neq Q$).\n",
    "\n",
    "The goal of this notebook is to investigate the phenomena of the neural net two-sample test, which basically uses the time to learn the two distributions as the statistical test.  So what we will do is\n",
    "\n",
    "1.  Create a hard two sample testing problem by creating $A$ and $B$ from some $P$ and $Q$ that are very hard to differentiate.\n",
    "2.  Create a base neural net class (with last layer as either symmetrized initialization, 0 initialization, or random).  In each case, we need to run a permutation test and plot figures of the data with the color map of the outputs and witness function function values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow\n",
    "pip uninstall tensorflow-probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tqdm import trange\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, Manager\n",
    "import datasets\n",
    "import gc\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LatentDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LinearDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will let $P$ and $Q$ both be Gaussian mixture models given by\n",
    "\n",
    "$$P = \\sum_{i=1}^2 \\frac{1}{2} \\mathcal{N}( \\mu_i^h , I_d )$$\n",
    "$$Q = \\sum_{i=1}^2 \\frac{1}{2} \\mathcal{N}\\Bigg( \\mu_i^h , \\begin{bmatrix}\n",
    "    1 & \\Delta_i^h & 0_{d-2} \\\\ \\Delta_i^h & 1 & 0_{d-2} \\\\ 0_{d-2}^\\top & 0_{d-2}^\\top & I_{d-2}\n",
    "\\end{bmatrix} \\Bigg),$$\n",
    "where $\\mu_1^h = 0_d$, $\\mu_2^h = 0.5 * \\mathbf{1}_d$, $\\Delta_1^h = 0.5$, and $\\Delta_2^h = -0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Constructor(object):\n",
    "    def __init__(self, trainN=5000, testN=1000, d=20, batch_sz=32):\n",
    "        # set hyperparams\n",
    "        self.trainN = trainN\n",
    "        self.testN = testN\n",
    "        self.N = self.trainN + self.testN\n",
    "        self.d = d\n",
    "        self.batch_sz = batch_sz\n",
    "        \n",
    "        # mean vectors\n",
    "        self.mu_1 = tf.zeros(self.d)\n",
    "        self.mu_2 = 0.5*tf.ones(d)\n",
    "        # covariance offsets\n",
    "        delta_1 = 0.5\n",
    "        delta_2 = -0.5\n",
    "        # define covariances\n",
    "        self.I = tf.eye(self.d)\n",
    "        self.sig_1 = tf.linalg.LinearOperatorBlockDiag([\n",
    "                        tf.linalg.LinearOperatorFullMatrix(tf.constant([ [1, delta_1], [delta_1, 1] ])),\n",
    "                        tf.linalg.LinearOperatorFullMatrix(tf.eye(d-2))])\n",
    "        self.sig_1 = self.sig_1.to_dense()\n",
    "        self.sig_2 = tf.linalg.LinearOperatorBlockDiag([\n",
    "                        tf.linalg.LinearOperatorFullMatrix(tf.constant([ [1, delta_2], [delta_2, 1] ])),\n",
    "                        tf.linalg.LinearOperatorFullMatrix(tf.eye(d-2))])\n",
    "        self.sig_2 = self.sig_2.to_dense()\n",
    "        # categorical sampling methods\n",
    "        self.bernoulli_dist = tfp.distributions.Bernoulli(probs=[0.5])\n",
    "        # construct gaussian distributions\n",
    "        self.gauss_mix_1 = [tfp.distributions.MultivariateNormalFullCovariance(\n",
    "                            loc=self.mu_1, covariance_matrix=self.I), \n",
    "                       tfp.distributions.MultivariateNormalFullCovariance(\n",
    "                            loc=self.mu_2, covariance_matrix=self.I)]\n",
    "        self.gauss_mix_2 = [tfp.distributions.MultivariateNormalFullCovariance(\n",
    "                            loc=self.mu_1, covariance_matrix=self.sig_1), \n",
    "                       tfp.distributions.MultivariateNormalFullCovariance(\n",
    "                            loc=self.mu_2, covariance_matrix=self.sig_2)]\n",
    "\n",
    "    def constructData(self):\n",
    "        # figure out how many to sample from first Gaussian dist vs second Gaussian dist\n",
    "        cat_dist_P = self.bernoulli_dist.sample(self.N)\n",
    "        cat_dist_Q = self.bernoulli_dist.sample(self.N)\n",
    "        \n",
    "        samp_dist_P = tf.math.reduce_sum(cat_dist_P)\n",
    "        dset_A_X = tf.concat([self.gauss_mix_1[0].sample(samp_dist_P),\n",
    "                              self.gauss_mix_1[1].sample(self.N-samp_dist_P)],\n",
    "                                 axis=0)\n",
    "        dset_A_Y = tf.expand_dims(tf.zeros(self.N),axis=1)\n",
    "        dset_A_X_prime = self.gauss_mix_1[0].sample(self.N)\n",
    "        \n",
    "        samp_dist_Q = tf.math.reduce_sum(cat_dist_Q)\n",
    "        dset_B_X = tf.concat([self.gauss_mix_2[0].sample(samp_dist_Q),\n",
    "                             self.gauss_mix_2[1].sample(self.N-samp_dist_Q)],\n",
    "                                 axis=0)\n",
    "        dset_B_Y = tf.expand_dims(tf.ones(self.N), axis=1)\n",
    "        dset_B_X_prime = self.gauss_mix_2[0].sample(self.N)\n",
    "        # aggregate data from A and B\n",
    "        dset_X = tf.concat([dset_A_X, dset_B_X], axis=0)\n",
    "        dset_Y = tf.concat([dset_A_Y, dset_B_Y], axis=0)\n",
    "\n",
    "        # prime dataset\n",
    "        dset_X_prime = tf.concat([dset_A_X_prime, dset_B_X_prime], axis=0)\n",
    "        dset_Y_prime = tf.concat([dset_A_Y, dset_B_Y], axis=0)\n",
    "\n",
    "        # actually make dataset\n",
    "        dset = tf.data.Dataset.from_tensor_slices((dset_X,\n",
    "                        dset_Y)).shuffle(2*self.N).shuffle(2*self.N)\n",
    "        dset_Train = dset.take(self.trainN*2)\n",
    "        dset_Train = dset_Train.batch(self.batch_sz)\n",
    "\n",
    "        dset_Test = dset.skip(self.trainN*2)\n",
    "        dset_Test = dset_Test.batch(self.batch_sz)\n",
    "\n",
    "        # make prime dataset\n",
    "        dset_prime = tf.data.Dataset.from_tensor_slices((dset_X_prime,\n",
    "                        dset_Y_prime)).shuffle(2*self.N).shuffle(2*self.N)\n",
    "        dset_Train_prime = dset_prime.take(self.trainN*2)\n",
    "        dset_Train_prime = dset_Train_prime.batch(self.batch_sz)\n",
    "\n",
    "        dset_Test_prime = dset_prime.skip(self.trainN*2)\n",
    "        dset_Test_prime = dset_Test_prime.batch(self.batch_sz)\n",
    "        return dset_Train, dset_Test, dset_Train_prime, dset_Test_prime\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "We will create a neural network that is able to be initialized randomly, from zero, and in a symmetric manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class symmetricLastLayer_Init(tf.keras.initializers.Initializer):\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, shape, dtype=tf.float32, **kwargs):\n",
    "        input_shape = shape[0]\n",
    "        if input_shape % 2 == 0: # even input shape\n",
    "            init_weights = tf.concat( [tf.ones(input_shape//2,dtype=dtype),\n",
    "                                       -tf.ones(input_shape//2,dtype=dtype)],\n",
    "                                    axis=0)\n",
    "        else: # odd input shape\n",
    "            init_weights = tf.concat( [tf.ones(input_shape//2, dtype=dtype),\n",
    "                                      tf.constant([0.0], dtype=dtype),\n",
    "                                      -tf.ones(input_shape//2, dtype=dtype)],\n",
    "                                    axis=0)\n",
    "        return tf.expand_dims(init_weights,axis=1)\n",
    "    \n",
    "class symmetricPenultimate_Init(tf.keras.initializers.Initializer):\n",
    "    def __init__(self, mean, stddev):\n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "    \n",
    "    def __call__(self, shape, dtype=tf.float32, **kwargs):\n",
    "        output_shape = shape[1]\n",
    "        first_half_rows = tf.random.normal([shape[0],output_shape//2],\n",
    "                                mean=self.mean, stddev=self.stddev, dtype=dtype)\n",
    "        if output_shape % 2 == 0: # even output shape\n",
    "            init_weights = tf.concat( [first_half_rows,\n",
    "                                       first_half_rows],\n",
    "                                    axis=1)\n",
    "        else: # odd output shape\n",
    "            init_weights = tf.concat( [first_half_rows,\n",
    "                                        tf.ones([shape[0],1], dtype=dtype),\n",
    "                                       first_half_rows],\n",
    "                                    axis=1)\n",
    "        return init_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual neural net model\n",
    "class Logit_NN(keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim=768, L = 10, activation='relu',\n",
    "                     init_method='random',model_type='logit'):\n",
    "        super(Logit_NN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim # hidden_dim\n",
    "        self.L = L # number of layers\n",
    "        self.init_method = init_method # random, zeros, or symmetric\n",
    "        self.model_type = model_type\n",
    "        self.layers = [layers.Dense(self.hidden_dim,\n",
    "                            activation=activation) for j in range(L-2)]\n",
    "        penultimate_init = 'glorot_uniform'\n",
    "        if self.init_method == 'random':\n",
    "            last_init = 'glorot_uniform'\n",
    "        elif self.init_method == 'zeros':\n",
    "            last_init = tf.keras.initializers.Zeros()\n",
    "        else: # 'symmetric'\n",
    "            penultimate_init = symmetricPenultimate_Init(mean=0.0,\n",
    "                                            stddev=tf.math.sqrt(1/self.hidden_dim))\n",
    "            last_init = symmetricLastLayer_Init()\n",
    "        self.penultimate_init = penultimate_init\n",
    "        self.last_init = last_init\n",
    "        self.penultimate_layer = layers.Dense(self.hidden_dim/2,\n",
    "                                activation=activation,\n",
    "                                kernel_initializer=penultimate_init)\n",
    "        if self.model_type == 'logit':\n",
    "            self.last_layer = layers.Dense(1, activation='sigmoid',\n",
    "                                kernel_initializer=last_init)\n",
    "        else:\n",
    "            self.last_layer = layers.Dense(1, activation='linear',\n",
    "                                kernel_initializer=last_init)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # these will help calulate delta_j\n",
    "        outputs = [x]\n",
    "        intermediate_hadamard_prods = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, training=training)\n",
    "            outputs.append(x)\n",
    "            hadamard_out = tf.linalg.diag(tf.cast(x > 0, dtype=tf.float32))\n",
    "            intermediate_hadamard_prods.append(hadamard_out)\n",
    "        x = self.penultimate_layer(x, training=training)\n",
    "        outputs.append(x)\n",
    "        hadamard_out = tf.linalg.diag(tf.cast(x > 0, dtype=tf.float32))\n",
    "        intermediate_hadamard_prods.append(hadamard_out)\n",
    "        x = self.last_layer(x, training=training)\n",
    "        if self.init_method == 'symmetric' and self.model_type != 'logit':\n",
    "            x = x+(0.5)\n",
    "        return x, outputs, intermediate_hadamard_prods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training class with plotting\n",
    "\n",
    "Here, we create a training object that will be used to 1) train the neural net object above, 2) plot the procession of the neural net output as training ensues, and 3) apply a permutation test and plot the histogram for it.  We will apply this training class to the different model situations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, hidden_dim=50, L = 5, activation='relu',\n",
    "                    init_method='random',model_type='logit', lr = 0.001,\n",
    "                trainN=5000, testN=1000, d=20, batch_sz=32):\n",
    "        # first set model that will be trained\n",
    "        self.model_type = model_type\n",
    "        self.lr = lr\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.L = L\n",
    "        self.init_method = init_method\n",
    "        self.model_type = model_type\n",
    "        self.data_constructor = Data_Constructor(trainN=trainN, testN=testN,\n",
    "                                                d=d, batch_sz=batch_sz)\n",
    "        self.loss_fcn = tf.keras.losses.MeanSquaredError()\n",
    "        if self.model_type == 'logit':\n",
    "            self.loss_fcn = tf.keras.losses.BinaryCrossentropy()\n",
    "        self.optimizer = keras.optimizers.SGD(learning_rate=self.lr)\n",
    "        \n",
    "    def train_and_plot(self, num_epochs, num_tests, m_perm=1000, n_bins=50):\n",
    "        T_H0_mod1 = {j: [] for j in range(num_epochs)}\n",
    "        T_mod1 = {j: [] for j in range(num_epochs)}\n",
    "        T_H0_NTK1 = {j:[] for j in range(num_epochs)}\n",
    "        T_NTK1 = {j:[] for j in range(num_epochs)}\n",
    "        T_H0_mod2 = {j: [] for j in range(num_epochs)}\n",
    "        T_mod2 = {j: [] for j in range(num_epochs)}\n",
    "        T_H0_NTK2 = {j: [] for j in range(num_epochs)}\n",
    "        T_NTK2 = {j : [] for j in range(num_epochs)}\n",
    "        \n",
    "        for test in range(num_tests):\n",
    "            t1 = time.time()\n",
    "            # create model 1 for testing on Gaussian mixture data\n",
    "            # create model 2 for testing on regular Gaussian data\n",
    "            # form NTK for both model 1 and model 2 and their respective data\n",
    "            dset_train, dset_test, dset_train_prime, dset_test_prime = self.data_constructor.constructData()\n",
    "            model_1 = Logit_NN(hidden_dim=self.hidden_dim, L=self.L,\n",
    "                              init_method=self.init_method,model_type=self.model_type)\n",
    "            model_2 = Logit_NN(hidden_dim=self.hidden_dim, L=self.L,\n",
    "                              init_method=self.init_method,model_type=self.model_type)\n",
    "            #print('Running test', test, 'for model 1.....')\n",
    "            #T_H0_epoch_num_1, T_epoch_num_1 = self.train_model(model_1, dset_train,\n",
    "            #                                        dset_test, num_epochs=num_epochs,\n",
    "            #                                        m_perm=m_perm)\n",
    "            #print('Running test', test, 'for model 2....')\n",
    "            #T_H0_epoch_num_2, T_epoch_num_2 = self.train_model(model_2, dset_train_prime,\n",
    "            #                                        dset_test_prime, num_epochs=num_epochs,\n",
    "            #                                        m_perm=m_perm)\n",
    "            if self.init_method != 'zeros':\n",
    "                print('Running test', test, 'for NTK1....')\n",
    "                T_H0_ntk1_epoch_num, T_ntk1_epoch_num = self.train_NTK(model_1,\n",
    "                                                            num_epochs=num_epochs, m_perm=m_perm,\n",
    "                                                            mod_type=1)\n",
    "                print('Running test', test, 'for NTK2....')\n",
    "                T_H0_ntk2_epoch_num, T_ntk2_epoch_num = self.train_NTK(model_2,\n",
    "                                                            num_epochs=num_epochs, m_perm=m_perm,\n",
    "                                                            mod_type = 2)\n",
    "            for j in range(num_epochs):\n",
    "                #T_H0_mod1[j].extend(T_H0_epoch_num_1[j])\n",
    "                #T_mod1[j].append(T_epoch_num_1[j])\n",
    "                #T_H0_mod2[j].extend(T_H0_epoch_num_2[j])\n",
    "                #T_mod2[j].append(T_epoch_num_2[j])\n",
    "                if self.init_method != 'zeros':\n",
    "                    T_H0_NTK1[j].extend(T_H0_ntk1_epoch_num[j])\n",
    "                    T_NTK1[j].append(T_ntk1_epoch_num[j])\n",
    "                    T_H0_NTK2[j].extend(T_H0_ntk2_epoch_num[j])\n",
    "                    T_NTK2[j].append(T_ntk2_epoch_num[j])\n",
    "            del_T = time.time() - t1\n",
    "            print('Took', del_T, 'seconds to finish one test...')\n",
    "            print('Estimated hours to finish all tests:', del_T*(num_tests - test)/3600)\n",
    "            if test % 2 == 0:\n",
    "                clear_output(wait=True)\n",
    "        self.T_2_H0 = {'model on mixture data':[T_H0_mod1, T_mod1],\n",
    "                  'model on Gaussian data':[T_H0_mod2, T_mod2],\n",
    "                  'NTK on mixture data':[T_H0_NTK1, T_NTK1],\n",
    "                  'NTK on Gaussian data':[T_H0_NTK2, T_NTK2]}\n",
    "        for j in range(num_epochs):\n",
    "            for mod_key in self.T_2_H0.keys():\n",
    "                if (not self.init_method == 'zeros') and ('NTK' in mod_key):\n",
    "                    T_H0 = self.T_2_H0[mod_key][0]\n",
    "                    T = self.T_2_H0[mod_key][1]\n",
    "                    plt.hist(tf.concat(T_H0[j],axis=0).numpy(), bins=50,\n",
    "                             label='permutation tests',density=True)\n",
    "                    y_vals = tf.zeros(tf.concat(T[j],axis=0).shape,dtype=tf.float32)\n",
    "                    #plt.plot(tf.concat(T[j],axis=0).numpy(), y_vals ,color='r',marker='o',markersize=15)\n",
    "                    plt.hist(tf.concat(T[j],axis=0).numpy(), bins=3,label='actual',\n",
    "                                        density=True)\n",
    "                    plt.legend(loc='upper left')\n",
    "                    title_name = 'Permutation Test of '+mod_key+' for Epoch'+str(j)\n",
    "                    plt.title(title_name)\n",
    "                    plt.show()\n",
    "            \n",
    "    def train_model(self, model, train_set, test_set, num_epochs=10, m_perm=1000, plot_data=False):\n",
    "        T_H0_epoch_num = {j: [] for j in range(num_epochs)}\n",
    "        T_epoch_num = {j:0 for j in range(num_epochs)}\n",
    "        for j, epoch in enumerate(range(num_epochs)):\n",
    "            for k, data in enumerate(train_set):\n",
    "                features, labels = data\n",
    "                with tf.GradientTape() as tape:\n",
    "                    prediction, _, _ = model(features,training=True)\n",
    "                    loss = self.loss_fcn(prediction, labels)\n",
    "                grads = tape.gradient(loss,model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(list(zip(grads,\n",
    "                                            model.trainable_variables)))\n",
    "            feats_2_plot = []\n",
    "            predictions_2_plot = []\n",
    "            labels_2_plot = []\n",
    "            for k, data in enumerate(test_set):\n",
    "                # aggregate information for plotting\n",
    "                features, labels = data\n",
    "                feats_2_plot.append(features[:,:2])\n",
    "                labels_2_plot.append(labels)\n",
    "                prediction, _, _ = model(features, training=False)\n",
    "                predictions_2_plot.append(prediction)\n",
    "            # plot\n",
    "            plotting_feats = tf.concat(feats_2_plot, axis=0)\n",
    "            plotting_preds = tf.concat(predictions_2_plot, axis=0)\n",
    "            plotting_labels = tf.concat(labels_2_plot, axis=0)\n",
    "            if plot_data:\n",
    "                plt.scatter(plotting_feats.numpy()[:,0],\n",
    "                            plotting_feats.numpy()[:,1],\n",
    "                            c=plotting_preds.numpy())\n",
    "                plt.colorbar()\n",
    "                plt.title('Actual Predictions of Model')\n",
    "                plt.show()\n",
    "                # plotting for comparison to labels\n",
    "                plt.scatter(plotting_feats.numpy()[:,0],\n",
    "                            plotting_feats.numpy()[:,1],\n",
    "                            c=plotting_labels.numpy())\n",
    "                plt.colorbar()\n",
    "                plt.title('Labels for Dataset')\n",
    "                plt.show()\n",
    "            # plot permutation test\n",
    "            T_1 = tf.math.reduce_mean(tf.gather(plotting_preds,\n",
    "                                  tf.concat(tf.where(plotting_labels==1),axis=0)[:,0], axis=0))\n",
    "            T_0 = tf.math.reduce_mean(tf.gather(plotting_preds,\n",
    "                                  tf.concat(tf.where(plotting_labels==0),axis=0)[:,0], axis=0))\n",
    "            T = T_1 - T_0\n",
    "            # apply permutation test\n",
    "            T_H0 = []\n",
    "            for k in range(m_perm):\n",
    "                dummy_labels = tf.random.shuffle(plotting_labels)\n",
    "                T_1_dummy = tf.math.reduce_mean(tf.gather(plotting_preds,\n",
    "                                  tf.concat(tf.where(dummy_labels==1),axis=0)[:,0], axis=0))\n",
    "                T_0_dummy = tf.math.reduce_mean(tf.gather(plotting_preds,\n",
    "                                  tf.concat(tf.where(dummy_labels==0),axis=0)[:,0], axis=0))\n",
    "                T_H0.append(T_1_dummy - T_0_dummy)\n",
    "            T_H0_epoch_num[j].extend(T_H0)\n",
    "            T_epoch_num[j] += T\n",
    "        return T_H0_epoch_num, T_epoch_num\n",
    "            \n",
    "    def calc_reps(features, model):\n",
    "        # calculating the delta_j's and outputs\n",
    "        x, outputs, hadamard_outs = model(features)\n",
    "        delta_j = model.last_layer.weights[0]\n",
    "        deltas = [tf.squeeze(delta_j)]\n",
    "        delta_j = tf.matmul( model.penultimate_layer.weights[0],\n",
    "                               tf.matmul(hadamard_outs[-1], delta_j))\n",
    "        deltas.append(tf.squeeze(delta_j))\n",
    "        for j in range(model.L-3, -1, -1):\n",
    "            delta_j = tf.matmul(model.layers[j].weights[0],\n",
    "                                tf.matmul(hadamard_outs[j], delta_j))\n",
    "            deltas.append(tf.squeeze(delta_j))\n",
    "        deltas.reverse()\n",
    "        return x, outputs, deltas\n",
    "\n",
    "\n",
    "    def concat_reps(train_set, model):\n",
    "        outs = []\n",
    "        delta_data = []\n",
    "        for j, data in enumerate(train_set):\n",
    "            feats, label = data\n",
    "            x, outputs, deltas = Trainer.calc_reps(feats, model)\n",
    "            if len(outs) == 0:\n",
    "                outs = outputs\n",
    "                delta_data = deltas\n",
    "            else:\n",
    "                 for j in range(len(outs)):\n",
    "                        if j != len(outs) - 1:\n",
    "                            delta_data[j] = tf.concat([delta_data[j],deltas[j]],axis=0)\n",
    "                        outs[j] = tf.concat([outs[j],outputs[j]],axis=0)\n",
    "        return outs, delta_data\n",
    "\n",
    "    def form_NTK(outputs, deltas, nn_type='logit'):\n",
    "        ntk = 0\n",
    "        for k in range(len(outputs)):\n",
    "            if k == len(outputs)-1:\n",
    "                d_j = tf.matmul(tf.expand_dims(deltas[k],\n",
    "                                axis=0),\n",
    "                                tf.transpose(tf.expand_dims(deltas[k],\n",
    "                                                            axis=0)))\n",
    "            else:\n",
    "                d_j = tf.matmul(deltas[k],tf.transpose(deltas[k]))\n",
    "            x_j = tf.matmul(outputs[k], tf.transpose(outputs[k]))\n",
    "            ntk = ntk + d_j*x_j\n",
    "        if nn_type == 'logit':\n",
    "            ntk = (1/4)*ntk\n",
    "        # normalize ntk\n",
    "        k_hat = tf.linalg.tensor_diag_part(ntk)\n",
    "        k_hat = tf.expand_dims(1/tf.math.sqrt(tf.linalg.tensor_diag_part(ntk)),axis=1)\n",
    "        k_hat_prime = tf.linalg.matmul(k_hat, tf.transpose(k_hat))\n",
    "        ntk = ntk*k_hat_prime\n",
    "        return ntk\n",
    "    \n",
    "    def train_NTK(self,model, num_epochs, m_perm=1000, mod_type = 1):\n",
    "        dset_train, dset_test, dset_train_prime, dset_test_prime = self.data_constructor.constructData()\n",
    "        if mod_type == 2:\n",
    "            dset_train = dset_train_prime\n",
    "            dset_test = dset_test_prime\n",
    "        out, delta = Trainer.concat_reps(dset_train.concatenate(dset_test),model)\n",
    "        ntk = Trainer.form_NTK(out, delta, nn_type=self.model_type)\n",
    "        train_N = self.data_constructor.trainN*2\n",
    "        init = tf.random.normal([train_N , 1])\n",
    "        init = tf.zeros([train_N, 1])\n",
    "        # split up NTK into different components\n",
    "        ntk_train = ntk[:train_N, :train_N] # rows and columns from train\n",
    "        ntk_test = ntk[train_N:, :train_N] # rows from test and columns from train\n",
    "        T_H0_epoch_num = {j:[] for j in range(num_epochs)}\n",
    "        T_epoch_num = {j: 0 for j in range(num_epochs)}\n",
    "        D, V = np.linalg.eigh(ntk_train)\n",
    "        for j in range(num_epochs):\n",
    "            for k, batch in enumerate(dset_train):\n",
    "                feats, label = batch\n",
    "                batch_sz = feats.shape[0]\n",
    "                if self.model_type == 'logit':\n",
    "                    dy = label - tf.math.sigmoid(init[j*batch_sz:(j+1)*batch_sz])\n",
    "                else:\n",
    "                    dy = label - init[j*batch_sz: (j+1)*batch_sz]\n",
    "                gradstep = tf.linalg.matmul(tf.transpose(tf.gather(ntk_train,\n",
    "                                        list(range(j*batch_sz,\n",
    "                                        (j+1)*batch_sz)), axis=0 )), dy)\n",
    "                #print('gradstep:', gradstep)\n",
    "                # (1e-5)*\n",
    "                init = init - self.lr*gradstep\n",
    "            feats_2_plot = []\n",
    "            predictions_2_plot = []\n",
    "            labels_2_plot = []\n",
    "            for k, batch in enumerate(dset_test):\n",
    "                feats, label = batch\n",
    "                batch_sz = feats.shape[0]\n",
    "                labels_2_plot.append(label)\n",
    "            plotting_labels = tf.concat(labels_2_plot, axis=0)\n",
    "            coefs = tf.linalg.matmul(V,tf.linalg.matmul(tf.linalg.diag(1/D),\n",
    "                                                tf.linalg.matmul(V.T, init)))\n",
    "            plotting_preds = tf.matmul(ntk_test, coefs)\n",
    "            T_1 = tf.math.reduce_mean(tf.gather(plotting_preds,\n",
    "                                    tf.concat(tf.where(plotting_labels==1),\n",
    "                                             axis=0)[:,0], axis=0))\n",
    "            T_0 = tf.math.reduce_mean(tf.gather(plotting_preds,\n",
    "                                    tf.concat(tf.where(plotting_labels==0),\n",
    "                                             axis=0)[:,0], axis=0))\n",
    "            T = T_1 - T_0\n",
    "            # apply permutation test\n",
    "            T_H0 = []\n",
    "            for k in range(m_perm):\n",
    "                dummy_labels = tf.random.shuffle(plotting_labels)\n",
    "                T_1_dummy = tf.math.reduce_mean(tf.gather(plotting_preds,\n",
    "                                  tf.concat(tf.where(dummy_labels==1),axis=0)[:,0], axis=0))\n",
    "                T_0_dummy = tf.math.reduce_mean(tf.gather(plotting_preds,\n",
    "                                  tf.concat(tf.where(dummy_labels==0),axis=0)[:,0], axis=0))\n",
    "                T_H0.append(T_1_dummy - T_0_dummy)\n",
    "            T_H0_epoch_num[j].extend(T_H0)\n",
    "            T_epoch_num[j] += T\n",
    "        del dset_train, \n",
    "        return T_H0_epoch_num, T_epoch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams for series of tests\n",
    "num_epochs = 10\n",
    "L = 2\n",
    "activation='relu'\n",
    "lr = 0.001\n",
    "num_tests = 10\n",
    "m_perm = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests with Hidden Dim 4000 ( same as number of training points)\n",
    "\n",
    "We run tests the different initialization methods for each of logit vs. l2_loss cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 4000\n",
    "# 50, 4000, 80000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric Initialization\n",
    "### Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_obj = Trainer(hidden_dim=hidden_dim, L = L, activation=activation,\n",
    "            init_method='symmetric',model_type='logit', lr = lr,\n",
    "                trainN=2000, testN=500, d=20, batch_sz=32)\n",
    "trainer_obj.train_and_plot(num_epochs=num_epochs, \n",
    "                           num_tests=num_tests, m_perm=m_perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_obj = Trainer(hidden_dim=hidden_dim, L = L, activation=activation,\n",
    "            init_method='symmetric',model_type='l2', lr = lr,\n",
    "                trainN=2000, testN=500, d=20, batch_sz=32)\n",
    "trainer_obj.train_and_plot(num_epochs=num_epochs, \n",
    "                           num_tests=num_tests, m_perm=m_perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization\n",
    "### Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_obj = Trainer(hidden_dim=hidden_dim, L = L, activation=activation,\n",
    "            init_method='random',model_type='logit', lr = lr,\n",
    "                trainN=2000, testN=500, d=20, batch_sz=32)\n",
    "trainer_obj.train_and_plot(num_epochs=num_epochs, \n",
    "                           num_tests=num_tests, m_perm=m_perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_obj = Trainer(hidden_dim=hidden_dim, L = L, activation=activation,\n",
    "            init_method='random',model_type='l2', lr = lr,\n",
    "                trainN=2000, testN=500, d=20, batch_sz=32)\n",
    "trainer_obj.train_and_plot(num_epochs=num_epochs, \n",
    "                           num_tests=num_tests, m_perm=m_perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Initialization\n",
    "### Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_obj = Trainer(hidden_dim=hidden_dim, L = L, activation=activation,\n",
    "            init_method='zeros',model_type='logit', lr = 1e-2*lr,\n",
    "                trainN=2000, testN=500, d=20, batch_sz=32)\n",
    "trainer_obj.train_and_plot(num_epochs=num_epochs, \n",
    "                           num_tests=num_tests, m_perm=m_perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_obj = Trainer(hidden_dim=hidden_dim, L = L, activation=activation,\n",
    "            init_method='zeros',model_type='l2', lr = lr,\n",
    "                trainN=2000, testN=500, d=20, batch_sz=32)\n",
    "trainer_obj.train_and_plot(num_epochs=num_epochs, \n",
    "                           num_tests=num_tests, m_perm=m_perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests with hidden dim 80000\n",
    "\n",
    "We run tests the different initialization methods for each of logit vs. l2_loss cases with hidden dimension now 80000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 40000\n",
    "num_tests = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric Initialization\n",
    "### Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_obj = Trainer(hidden_dim=hidden_dim, L = L, activation=activation,\n",
    "            init_method='symmetric',model_type='logit', lr = lr,\n",
    "                trainN=2000, testN=500, d=20, batch_sz=32)\n",
    "trainer_obj.train_and_plot(num_epochs=num_epochs, \n",
    "                           num_tests=num_tests, m_perm=m_perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_obj = Trainer(hidden_dim=hidden_dim, L = L, activation=activation,\n",
    "            init_method='symmetric',model_type='l2', lr = lr,\n",
    "                trainN=2000, testN=500, d=20, batch_sz=32)\n",
    "trainer_obj.train_and_plot(num_epochs=num_epochs, \n",
    "                           num_tests=num_tests, m_perm=m_perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization\n",
    "### Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_obj = Trainer(hidden_dim=hidden_dim, L = L, activation=activation,\n",
    "            init_method='random',model_type='logit', lr = lr,\n",
    "                trainN=2000, testN=500, d=20, batch_sz=32)\n",
    "trainer_obj.train_and_plot(num_epochs=num_epochs, \n",
    "                           num_tests=num_tests, m_perm=m_perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_obj = Trainer(hidden_dim=hidden_dim, L = L, activation=activation,\n",
    "            init_method='random',model_type='l2', lr = lr,\n",
    "                trainN=2000, testN=500, d=20, batch_sz=32)\n",
    "trainer_obj.train_and_plot(num_epochs=num_epochs, \n",
    "                           num_tests=num_tests, m_perm=m_perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Initialization\n",
    "### Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_obj = Trainer(hidden_dim=hidden_dim, L = L, activation=activation,\n",
    "            init_method='zeros',model_type='logit', lr = 1e-2*lr,\n",
    "                trainN=2000, testN=500, d=20, batch_sz=32)\n",
    "trainer_obj.train_and_plot(num_epochs=num_epochs, \n",
    "                           num_tests=num_tests, m_perm=m_perm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer_obj = Trainer(hidden_dim=hidden_dim, L = L, activation=activation,\n",
    "            init_method='zeros',model_type='l2', lr = lr,\n",
    "                trainN=2000, testN=500, d=20, batch_sz=32)\n",
    "trainer_obj.train_and_plot(num_epochs=num_epochs, \n",
    "                           num_tests=num_tests, m_perm=m_perm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
