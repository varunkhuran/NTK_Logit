{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do:\n",
    "\n",
    "1. Given neural net, form the NTK and do gradient descent on the kernel regression.\n",
    "2. Consolidate the resampling of the data, the forming of the neural net and NTK, and training into same class.\n",
    "3. Perform the test that we have multiple times (~100 times) to get a smoothed version of the permutation test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Given two datasets $A$ and $B$, which come respectively from distributions $P$ and $Q$, two-sample tests are basically a statistical test that tries to answer the hypothesis $H_0: P = Q$ (or reject it in favor of $H_1: P \\neq Q$).\n",
    "\n",
    "The goal of this notebook is to investigate the phenomena of the neural net two-sample test, which basically uses the time to learn the two distributions as the statistical test.  So what we will do is\n",
    "\n",
    "1.  Create a hard two sample testing problem by creating $A$ and $B$ from some $P$ and $Q$ that are very hard to differentiate.\n",
    "2.  Create a base neural net class (with last layer as either symmetrized initialization, 0 initialization, or random).  In each case, we need to run a permutation test and plot figures of the data with the color map of the outputs and witness function function values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "# convert code to pytorch\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, Manager\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import gc\n",
    "from IPython.display import clear_output\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LatentDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LinearDA\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_probability as tfp\n",
    "# from tensorflow.data import Dataset\n",
    "# from tensorflow import keras\n",
    "# from keras import layers\n",
    "# import tensorflow_datasets as tfds\n",
    "# import tensorflow_transform as tft\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tensorflow.keras.datasets import mnist\n",
    "# from tqdm import trange\n",
    "# import datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will let $P$ and $Q$ both be Gaussian mixture models given by\n",
    "\n",
    "$$P = \\sum_{i=1}^2 \\frac{1}{2} \\mathcal{N}( \\mu_i^h , I_d )$$\n",
    "$$Q = \\sum_{i=1}^2 \\frac{1}{2} \\mathcal{N}\\Bigg( \\mu_i^h , \\begin{bmatrix}\n",
    "    1 & \\Delta_i^h & 0_{d-2} \\\\ \\Delta_i^h & 1 & 0_{d-2} \\\\ 0_{d-2}^\\top & 0_{d-2}^\\top & I_{d-2}\n",
    "\\end{bmatrix} \\Bigg),$$\n",
    "where $\\mu_1^h = 0_d$, $\\mu_2^h = 0.5 * \\mathbf{1}_d$, $\\Delta_1^h = 0.5$, and $\\Delta_2^h = -0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Constructor(object):\n",
    "    def __init__(self, trainN=5000, testN=1000, d=20, batch_sz=32):\n",
    "        # set hyperparams\n",
    "        self.trainN = trainN\n",
    "        self.testN = testN\n",
    "        self.N = self.trainN + self.testN\n",
    "        self.d = d\n",
    "        self.batch_sz = batch_sz\n",
    "        \n",
    "        # mean vectors\n",
    "        self.mu_1 = torch.zeros(self.d)\n",
    "        self.mu_2 = 0.5*torch.ones(self.d)\n",
    "        # covariance offsets\n",
    "        delta_1 = 0.5\n",
    "        delta_2 = -0.5\n",
    "        # delta_1 = 0.7\n",
    "        # delta_2 = -0.7\n",
    "        # define covariances\n",
    "        self.I = torch.eye(self.d)\n",
    "        self.sig_1 = torch.block_diag( torch.Tensor( [ [1, delta_1], [delta_1, 1] ]), \n",
    "                                    torch.eye(self.d-2) )\n",
    "        #self.sig_1 = self.sig_1.to_dense()\n",
    "        self.sig_2 = torch.block_diag(  torch.Tensor( [ [1, delta_2], [delta_2, 1] ] ),\n",
    "                                    torch.eye(self.d-2) )\n",
    "        #self.sig_2 = self.sig_2.to_dense()\n",
    "        # construct gaussian distributions\n",
    "        self.gauss_mix_1 = [MultivariateNormal(self.mu_1, self.I), \n",
    "                            MultivariateNormal(self.mu_2, self.I)]\n",
    "        self.gauss_mix_2 = [MultivariateNormal(self.mu_1, self.sig_1), \n",
    "                            MultivariateNormal(self.mu_2, self.sig_2)]\n",
    "        \n",
    "\n",
    "    def constructData(self, label_neg_one=False, shuffle=True):\n",
    "        # figure out how many to sample from first Gaussian dist vs second Gaussian dist\n",
    "        cat_dist_P = torch.bernoulli(0.5*torch.ones(self.N))\n",
    "        cat_dist_Q = torch.bernoulli(0.5*torch.ones(self.N))\n",
    "        \n",
    "        # get samples from both mixture model 1 and 2\n",
    "        samp_dist_P = torch.sum(cat_dist_P) # decides number of samples from first gaussian vs second for mixture model 1\n",
    "        dset_A_X = torch.concat([self.gauss_mix_1[0].sample(torch.Size([int(samp_dist_P.item())])),\n",
    "                                 self.gauss_mix_1[1].sample(torch.Size([int(self.N-samp_dist_P.item())]))],\n",
    "                                 axis=0)\n",
    "        dset_A_Y = torch.ones((self.N,1)) # labels are 0\n",
    "        dset_A_X_prime = self.gauss_mix_1[0].sample(torch.Size([self.N]))\n",
    "        \n",
    "        samp_dist_Q = torch.sum(cat_dist_Q)\n",
    "        dset_B_X = torch.concat([self.gauss_mix_2[0].sample(torch.Size([int(samp_dist_Q.item())])),\n",
    "                                 self.gauss_mix_2[1].sample(torch.Size([int(self.N - samp_dist_Q.item())]))],\n",
    "                                 axis=0)\n",
    "        dset_B_Y = torch.zeros((self.N,1))\n",
    "        if label_neg_one: dset_B_Y = - torch.ones((self.N,1))\n",
    "        dset_B_X_prime = self.gauss_mix_2[0].sample(torch.Size([self.N]))\n",
    "        \n",
    "        # aggregate data from A and B\n",
    "        dset_X = torch.concat([dset_A_X, dset_B_X], axis=0)\n",
    "        dset_Y = torch.concat([dset_A_Y, dset_B_Y], axis=0)\n",
    "        dset = torch.concat([dset_X, dset_Y],axis=1)\n",
    "        dset = dset[torch.randperm(dset.shape[0])] # shuffle the batches\n",
    "        # create train and test datasets and DataLoaders\n",
    "        self.dset = dset\n",
    "        dset_train = dset[:self.trainN*2]\n",
    "        dset_test = dset[self.trainN*2:]\n",
    "        dl = torch.utils.data.DataLoader(dset, batch_size=self.batch_sz, shuffle=shuffle)\n",
    "        dl_train = torch.utils.data.DataLoader(dset_train, batch_size=self.batch_sz, shuffle=shuffle)\n",
    "        dl_test = torch.utils.data.DataLoader(dset_test, batch_size=self.batch_sz, shuffle=shuffle)\n",
    "                \n",
    "        # prime dataset\n",
    "        # aggregate from A and B\n",
    "        dset_X_prime = torch.concat([dset_A_X_prime, dset_B_X_prime], axis=0)\n",
    "        dset_Y_prime = torch.concat([dset_A_Y, dset_B_Y], axis=0)\n",
    "        dset_prime = torch.concat([dset_X_prime, dset_Y_prime],axis=1)\n",
    "        dset_prime = dset_prime[torch.randperm(dset_prime.shape[0])]\n",
    "        self.dset_prime = dset_prime\n",
    "        dset_prime_train = dset_prime[:self.trainN*2]\n",
    "        dset_prime_test = dset_prime[self.trainN*2:]\n",
    "        dl_prime = torch.utils.data.DataLoader(dset_prime, batch_size=self.batch_sz, shuffle=shuffle)\n",
    "        dl_prime_train = torch.utils.data.DataLoader(dset_prime_train, batch_size=self.batch_sz, shuffle=shuffle)\n",
    "        dl_prime_test = torch.utils.data.DataLoader(dset_prime_test, batch_size=self.batch_sz, shuffle=shuffle)\n",
    "        return dl, dl_train, dl_test, dl_prime, dl_prime_train, dl_prime_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dc = Data_Constructor(trainN=5000, testN=1000, d=20, batch_sz=32)\n",
    "dl, dl_train, dl_test, dl_prime, dl_prime_train, dl_prime_test = dc.constructData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(dc.dset[:,0], dc.dset[:,1], c = dc.dset[:,-1])\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "We will create a neural network that is able to be initialized randomly, from zero, and in a symmetric manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual NN model\n",
    "class Logit_NN(nn.Module):\n",
    "    def __init__(self, d=4, hidden_dim=768, L = 10, activation='relu', model_type='logit', symmetric=True):\n",
    "        super(Logit_NN, self).__init__()\n",
    "        self.d = d\n",
    "        self.hidden_dim = hidden_dim # hidden_dim\n",
    "        self.L = L # number of layers\n",
    "        self.model_type = model_type\n",
    "        self.symmetric = symmetric\n",
    "        \n",
    "        self.nonLin = nn.Identity()\n",
    "        if activation == 'relu':\n",
    "            self.nonLin = nn.ReLU()\n",
    "        elif activation == 'logistic':\n",
    "            self.nonLin = nn.Sigmoid()\n",
    "        self.layers = nn.ModuleList()\n",
    "        curr_dim = self.d\n",
    "        for j in range(self.L - 2):\n",
    "            if j == 0: layer = nn.Sequential( nn.Linear(self.d, self.hidden_dim), self.nonLin )\n",
    "            else: layer = nn.Sequential( nn.Linear(self.hidden_dim, self.hidden_dim), self.nonLin )\n",
    "            self.layers.append(layer)\n",
    "            curr_dim = self.hidden_dim\n",
    "        \n",
    "        self.penultimate_layer = nn.Sequential( nn.Linear(curr_dim, self.hidden_dim), self.nonLin )\n",
    "        self.last_layer = nn.Linear( self.hidden_dim, 1 )\n",
    "        half_w = torch.nn.init.kaiming_normal_(self.penultimate_layer[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        if self.symmetric:\n",
    "            pen_shape = self.penultimate_layer[0].weight.shape\n",
    "            in_shape = pen_shape[1]\n",
    "            out_shape = pen_shape[0]\n",
    "            half_w = half_w.data[:pen_shape[0]//2, :]\n",
    "            #half_w = torch.randn( (pen_shape[0]//2, in_shape) )\n",
    "            # define weights for \n",
    "            if out_shape % 2 == 0:\n",
    "                new_pen_w = torch.concat( [half_w, half_w],axis=0 )\n",
    "            else:\n",
    "                new_pen_w = torch.concat([half_w, \n",
    "                                      torch.ones( 1, in_shape ),\n",
    "                                      half_w], axis=0)\n",
    "            ll_out_shape = pen_shape[0]\n",
    "            if out_shape % 2 == 0:\n",
    "                last_w = torch.concat([torch.ones( 1, out_shape // 2 ), \n",
    "                              -torch.ones( 1, out_shape // 2 )],axis=1)\n",
    "            else:\n",
    "                last_w = torch.concat([torch.ones( 1, out_shape // 2 ), \n",
    "                                        torch.Tensor([[0.0]]),\n",
    "                                       -torch.ones( 1, out_shape // 2 )],axis=1)\n",
    "            last_w = 1/torch.sqrt(torch.Tensor([out_shape])) * last_w\n",
    "            \n",
    "            with torch.no_grad():  # To avoid tracking gradients\n",
    "                self.penultimate_layer[0].weight.copy_(new_pen_w)\n",
    "                pen_bias = torch.zeros_like(self.penultimate_layer[0].bias)\n",
    "                self.penultimate_layer[0].bias.copy_(pen_bias)\n",
    "                self.last_layer.weight.copy_(last_w)\n",
    "                self.last_layer.bias.copy_(torch.zeros_like(self.last_layer.bias))\n",
    "\n",
    "        if self.model_type == 'logit':\n",
    "            self.last_layer = nn.Sequential( self.last_layer, nn.Sigmoid() )\n",
    "\n",
    "        #self.model = nn.Sequential( self.layers, self.penultimate_layer, self.last_layer )\n",
    "\n",
    "    def forward(self, x, training=False):\n",
    "        # these help calculate delta_j\n",
    "        outputs = [x]\n",
    "        intermediate_hadamard_prods = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            outputs.append(x)\n",
    "            hadamard_out = (x>0).float()\n",
    "            intermediate_hadamard_prods.append(hadamard_out)\n",
    "        x = self.penultimate_layer(x)\n",
    "        outputs.append(x)\n",
    "        hadamard_out = (x>0).float()\n",
    "        intermediate_hadamard_prods.append(hadamard_out)\n",
    "        x = self.last_layer(x)\n",
    "        # if self.init_method == 'symmetric' and self.model_type != 'logit':\n",
    "        #     x = x+(0.5)\n",
    "        return x, outputs, intermediate_hadamard_prods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training class with plotting\n",
    "\n",
    "Here, we create a training object that will be used to 1) train the neural net object above, 2) plot the procession of the neural net output as training ensues, and 3) apply a permutation test and plot the histogram for it.  We will apply this training class to the different model situations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training and plotting function for NN\n",
    "# make training and plotting function for NTK (almost done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, hidden_dim=50, L = 5, activation='relu',\n",
    "                 model_type='logit', lr = 0.001, trainN=5000, \n",
    "                 testN=1000, d=20, batch_sz=32, thresh=95, ratio=None):\n",
    "        # first set model that will be trained\n",
    "        self.d = d\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.L = L\n",
    "        self.activation = activation\n",
    "        self.model_type = model_type\n",
    "        self.lr = lr\n",
    "        self.trainN = trainN\n",
    "        self.testN = testN\n",
    "        self.batch_sz = batch_sz\n",
    "        self.thresh = thresh\n",
    "        self.ratio = ratio\n",
    "        if self.ratio is not None: self.hidden_dim = int(self.ratio*2*self.trainN)\n",
    "        self.dc = Data_Constructor(trainN=self.trainN, testN=self.testN, d=self.d, batch_sz=self.batch_sz)\n",
    "        self.loss_fcn = mse_loss = nn.MSELoss() #tf.keras.losses.MeanSquaredError()\n",
    "        if self.model_type == 'logit':\n",
    "            self.loss_fcn = nn.BCELoss()\n",
    "        self.create_directory()\n",
    "\n",
    "    def makedir(self, path):\n",
    "        if not os.path.exists(path): os.mkdir(path)\n",
    "    \n",
    "    def create_directory(self,):\n",
    "        self.config = {'ratio':self.ratio,\n",
    "                  'hidden_dim':self.hidden_dim,\n",
    "                  'd':self.d,\n",
    "                  'L':self.L,\n",
    "                  'activation':self.activation,\n",
    "                  'model_type':self.model_type,\n",
    "                  'lr':self.lr,\n",
    "                  'trainN':self.trainN,\n",
    "                  'testN':self.testN,\n",
    "                  'batch_sz':self.batch_sz,\n",
    "                  'thresh':self.thresh\n",
    "                 }\n",
    "        # make directories\n",
    "        self.makedir('Experiments')        \n",
    "        self.base = os.path.join('Experiments', self.model_type+'_'+str(self.hidden_dim))\n",
    "        self.makedir(self.base)\n",
    "        # plot directory\n",
    "        # self.plot_path = os.path.join(self.base, 'plots')\n",
    "        # self.makedir(self.plot_path)\n",
    "        # save config\n",
    "        self.config_path = os.path.join(self.base, 'config')\n",
    "        with open(self.config_path, 'w', encoding='utf8') as json_file: json.dump(self.config, json_file)\n",
    "    \n",
    "    def train_and_plot(self, num_epochs, num_tests, m_perm=1000, n_bins=50, plot_data=False, ntk=False):\n",
    "        # instantiate everything\n",
    "        T_H0_mod1 = {j: [] for j in range(num_epochs)}\n",
    "        T_mod1 = {j: [] for j in range(num_epochs)}\n",
    "        T_H0_NTK1 = {j:[] for j in range(num_epochs)}\n",
    "        T_NTK1 = {j:[] for j in range(num_epochs)}\n",
    "        T_H0_mod2 = {j: [] for j in range(num_epochs)}\n",
    "        T_mod2 = {j: [] for j in range(num_epochs)}\n",
    "        T_H0_NTK2 = {j: [] for j in range(num_epochs)}\n",
    "        T_NTK2 = {j : [] for j in range(num_epochs)}\n",
    "        self.mod1_power = {j:[] for j in range(num_epochs)}\n",
    "        self.mod2_power = {j:[] for j in range(num_epochs)}\n",
    "        \n",
    "        for test in range(num_tests):\n",
    "            t1 = time.time()\n",
    "            # create model 1 for testing on Gaussian mixture data\n",
    "            # create model 2 for testing on regular Gaussian data\n",
    "            # form NTK for both model 1 and model 2 and their respective data\n",
    "            model1 = Logit_NN(d=self.d, hidden_dim=self.hidden_dim, L=self.L, model_type=self.model_type, symmetric=True)\n",
    "            model2 = Logit_NN(d=self.d, hidden_dim=self.hidden_dim, L=self.L, model_type=self.model_type, symmetric=True)\n",
    "        \n",
    "            # run \n",
    "            label_neg_one = True\n",
    "            if self.model_type == 'logit': label_neg_one = False\n",
    "            dset, \\\n",
    "                dset_train, \\\n",
    "                dset_test, \\\n",
    "                dset_prime, \\\n",
    "                dset_train_prime, \\\n",
    "                dset_test_prime = self.dc.constructData(label_neg_one=label_neg_one, shuffle=True)\n",
    "            \n",
    "            print('Test', test, '| model 1 | hidden_dim', self.hidden_dim,'| ratio', self.ratio)\n",
    "            T1_epoch_num, T1_H0_epoch_num = self.train_model(model1, dset_train, dset_test, num_epochs=num_epochs, \n",
    "                                                           m_perm=m_perm, plot_data=plot_data)\n",
    "            print('Test', test, '| model 2 | hidden_dim', self.hidden_dim,'| ratio', self.ratio)\n",
    "            T2_epoch_num, T2_H0_epoch_num = self.train_model(model2, dset_train_prime, dset_test_prime, \n",
    "                                                           num_epochs=num_epochs, m_perm=m_perm, plot_data=plot_data)\n",
    "            if ntk:\n",
    "                print('Test', test, '| NTK 1 | hidden_dim', self.hidden_dim,'| ratio', self.ratio)\n",
    "                T_ntk1_epoch_num, T_H0_ntk1_epoch_num = self.train_NTK(model1,\n",
    "                                                            num_epochs=num_epochs, m_perm=m_perm,\n",
    "                                                            mod_type=1)\n",
    "                print('Test', test, '| NTK 2 | hidden_dim', self.hidden_dim,'| ratio', self.ratio)\n",
    "                T_ntk2_epoch_num, T_H0_ntk2_epoch_num = self.train_NTK(model2,\n",
    "                                                            num_epochs=num_epochs, m_perm=m_perm,\n",
    "                                                            mod_type=2)\n",
    "            for j in range(num_epochs):\n",
    "                # get test power data\n",
    "                t1_pctile = np.percentile(T1_H0_epoch_num[j], q=self.thresh)\n",
    "                t2_pctile = np.percentile(T2_H0_epoch_num[j], q=self.thresh)\n",
    "                self.mod1_power[j].append(int(t1_pctile < T1_epoch_num[j]))\n",
    "                self.mod2_power[j].append(int(t2_pctile < T2_epoch_num[j]))\n",
    "                T_H0_mod1[j].extend(T1_H0_epoch_num[j])\n",
    "                T_mod1[j].append(T1_epoch_num[j])                \n",
    "                T_H0_mod2[j].extend(T2_H0_epoch_num[j])\n",
    "                T_mod2[j].append(T2_epoch_num[j])\n",
    "                if ntk:\n",
    "                    T_H0_NTK1[j].extend(T_H0_ntk1_epoch_num[j])\n",
    "                    T_NTK1[j].append(T_ntk1_epoch_num[j])\n",
    "                    T_H0_NTK2[j].extend(T_H0_ntk2_epoch_num[j])\n",
    "                    T_NTK2[j].append(T_ntk2_epoch_num[j])\n",
    "            del_T = time.time() - t1\n",
    "            print('Took', del_T, 'seconds to finish one test...')\n",
    "            print('Estimated hours to finish all tests:', del_T*(num_tests - test)/3600)\n",
    "            if test % 2 == 0:\n",
    "                clear_output(wait=True)\n",
    "        self.T_2_H0 = {'model on mixture data':[T_H0_mod1, T_mod1],\n",
    "                  'model on Gaussian data':[T_H0_mod2, T_mod2],\n",
    "                  'NTK on mixture data':[T_H0_NTK1, T_NTK1],\n",
    "                  'NTK on Gaussian data':[T_H0_NTK2, T_NTK2]}\n",
    "        for j in range(num_epochs):\n",
    "            for mod_key in self.T_2_H0.keys():\n",
    "                # add portion to save data\n",
    "                if (ntk and ('NTK' in mod_key)) or ('NTK' not in mod_key):\n",
    "                    T_H0 = self.T_2_H0[mod_key][0]\n",
    "                    T = self.T_2_H0[mod_key][1]\n",
    "                    plt.hist(torch.stack(T_H0[j]).numpy(), bins=50,\n",
    "                             label='permutation tests',density=True)\n",
    "                    #plt.plot(tf.concat(T[j],axis=0).numpy(), y_vals ,color='r',marker='o',markersize=15)\n",
    "                    plt.hist(torch.Tensor(T[j]).numpy(), bins=3,label='actual', density=True)\n",
    "                    plt.legend(loc='upper left')\n",
    "                    title_name = 'Permutation Test of '+mod_key+' for Epoch'+str(j)\n",
    "                    plt.title(title_name)\n",
    "                    # save plot\n",
    "                    mod_type = 'model' if 'model' in mod_key else 'NTK'\n",
    "                    data_type = 'mixture' if 'mixture' in mod_key else 'Gaussian'\n",
    "                    d_path = os.path.join(self.base, data_type)\n",
    "                    self.makedir(d_path)\n",
    "                    m_path = os.path.join(d_path, mod_type)\n",
    "                    self.makedir(m_path)\n",
    "                    plot_path = os.path.join(m_path, 'epoch_'+str(j)+'.png')\n",
    "                    plt.savefig(plot_path)\n",
    "                    plt.show()\n",
    "        return self.mod1_power, self.mod2_power\n",
    "\n",
    "    def calc_reps(features, model):\n",
    "        # calculating the delta_j's and outputs\n",
    "        x, outputs, hadamard_outs = model(features)\n",
    "        if model.model_type == 'logit': delta_j = torch.Tensor([1/4])\n",
    "        else: delta_j = torch.Tensor([1.0])\n",
    "        deltas = [delta_j]\n",
    "        try: del_x = delta_j * model.last_layer[0].weight\n",
    "        except: del_x = delta_j * model.last_layer.weight\n",
    "        delta_j = (hadamard_outs[-1] * del_x)\n",
    "        deltas.append(delta_j)\n",
    "        del_x = delta_j @ model.penultimate_layer[0].weight\n",
    "        for j in range(model.L-3, -1, -1):\n",
    "            delta_j = hadamard_outs[j] * del_x\n",
    "            deltas.append(torch.squeeze(delta_j))\n",
    "            del_x = delta_j @ model.layers[j][0].weight\n",
    "        deltas.reverse()\n",
    "        return x, outputs, deltas\n",
    "    \n",
    "    def concat_reps(train_set, model):\n",
    "        outs = []\n",
    "        delta_data = []\n",
    "        for j, data in enumerate(train_set):\n",
    "            features = data[:, :-1]\n",
    "            labels = data[:, -1]\n",
    "            x, outputs, deltas = Trainer.calc_reps(features, model)\n",
    "            if len(outs) == 0:\n",
    "                outs = outputs\n",
    "                delta_data = deltas\n",
    "            else:\n",
    "                 for j in range(len(outs)):\n",
    "                        if j < len(deltas) - 1:\n",
    "                            delta_data[j] = torch.concat([delta_data[j],deltas[j]],axis=0)\n",
    "                        outs[j] = torch.concat([outs[j],outputs[j]],axis=0)\n",
    "        return outs, delta_data\n",
    "    \n",
    "    def form_NTK(outputs, deltas, nn_type='logit'):\n",
    "        ntk = 0\n",
    "        for k in range(len(deltas)):\n",
    "            x_j = (outputs[k] @ outputs[k].T) + 1\n",
    "            d_j = deltas[k] @ deltas[k].T\n",
    "            ntk = ntk + d_j * x_j\n",
    "        if nn_type == 'logit':\n",
    "            ntk = (1/4)*ntk\n",
    "        # normalize ntk\n",
    "        k_hat = torch.diagonal(ntk)\n",
    "        k_hat = (1/torch.sqrt(k_hat)).unsqueeze(1)\n",
    "        k_hat_prime = k_hat @ k_hat.T\n",
    "        ntk = ntk * k_hat_prime\n",
    "        return ntk\n",
    "\n",
    "    def train_NTK(self, model, num_epochs, m_perm=1000, mod_type = 1):\n",
    "        # Creates and trains an NTK model and later runs a permutation test\n",
    "        print('Constructing data...')\n",
    "        label_neg_one = True\n",
    "        if self.model_type == 'logit': label_neg_one = False\n",
    "        dset, \\\n",
    "            dset_train, \\\n",
    "            dset_test, \\\n",
    "            dset_prime, \\\n",
    "            dset_train_prime, \\\n",
    "            dset_test_prime = self.dc.constructData(label_neg_one=label_neg_one, \n",
    "                                                       shuffle=False)\n",
    "        if mod_type == 2:\n",
    "            dset = dset_prime\n",
    "            dset_train = dset_train_prime\n",
    "            dset_test = dset_test_prime\n",
    "        # get concatenated representations for dataset\n",
    "        print('Calculating NTK...')\n",
    "        out, delta = Trainer.concat_reps(dset, model)\n",
    "        ntk = Trainer.form_NTK(out, delta, nn_type=self.model_type)\n",
    "        train_N = self.dc.trainN*2\n",
    "        init = torch.randn((train_N,1))\n",
    "        init = torch.zeros((train_N,1))\n",
    "        # split up NTK into different components\n",
    "        ntk_train = ntk[:train_N, :train_N] # rows and columns from train\n",
    "        ntk_test = ntk[train_N:, :train_N] # rows from test and columns from train\n",
    "        \n",
    "        T_H0_epoch_num = {epoch:[] for epoch in range(num_epochs)}\n",
    "        T_epoch_num = {epoch: 0 for epoch in range(num_epochs)}\n",
    "        print('Applying eigendecomp to NTK....')\n",
    "        D, V = np.linalg.eigh(ntk_train.detach().numpy())\n",
    "        D = torch.Tensor(D)\n",
    "        V = torch.Tensor(V)\n",
    "        \n",
    "        label_p = 1\n",
    "        label_q = -1\n",
    "        if self.model_type == 'logit':\n",
    "            label_p = 1\n",
    "            label_q = 0\n",
    "        \n",
    "        #print('Starting training...')\n",
    "        with torch.no_grad():\n",
    "            for epoch in range(num_epochs):\n",
    "                labels_2_plot = []\n",
    "                for k, batch in enumerate(dset_test):\n",
    "                    batch = next(iter(dset_train))\n",
    "                    feats = batch[:,:-1]\n",
    "                    label = batch[:,-1:]\n",
    "                    batch_sz = feats.shape[0]\n",
    "                    labels_2_plot.append(label)\n",
    "                # get coefficients for prediction\n",
    "                coefs = V @ torch.diag(1/D) @ V.T @ init\n",
    "                plotting_preds = ntk_test @ coefs\n",
    "                if trainer.model_type == 'logit': plotting_preds = torch.sigmoid(plotting_preds)\n",
    "                # only keep labels that are up to the number of predictions\n",
    "                plotting_labels = torch.concat(labels_2_plot, axis=0)[:len(plotting_preds)]\n",
    "                T1 = plotting_preds[plotting_labels==label_p].mean() # label 1\n",
    "                T0 = plotting_preds[plotting_labels==label_q].mean() # label 0\n",
    "                T = T1 - T0\n",
    "                # apply permutation test\n",
    "                T_H0 = []\n",
    "                for k in range(m_perm):\n",
    "                    shuffled_indices = torch.randperm(plotting_labels.size(0))\n",
    "                    dummy_labels = plotting_labels[shuffled_indices]\n",
    "                    T_1_dummy = plotting_preds[dummy_labels==label_p].mean()\n",
    "                    T_0_dummy = plotting_preds[dummy_labels==label_q].mean()\n",
    "                    T_H0.append(T_1_dummy - T_0_dummy)\n",
    "                T_H0_epoch_num[epoch].extend(T_H0)\n",
    "                T_epoch_num[epoch] += T\n",
    "    \n",
    "                #clear_output(wait=True)\n",
    "                #print('Training epoch',epoch)\n",
    "                for k, batch in enumerate(dset_train):\n",
    "                    batch = next(iter(dset_train))\n",
    "                    feats = batch[:,:-1]\n",
    "                    label = batch[:,-1:]\n",
    "                    batch_sz = feats.shape[0]\n",
    "                    # indices to select\n",
    "                    idx1 = k*batch_sz\n",
    "                    idx2 = min((k+1)*batch_sz, train_N)\n",
    "                    if self.model_type == 'logit': dy = label - torch.sigmoid(init[idx1:idx2])\n",
    "                    else: dy = label - init[idx1:idx2]\n",
    "                    gradstep = ntk_train[idx1:idx2].T @ dy\n",
    "                    init = init - self.lr * gradstep\n",
    "                    \n",
    "        return T_epoch_num, T_H0_epoch_num\n",
    "    \n",
    "    def train_model(self, model, dset_train, dset_test, num_epochs=10, m_perm=1000, plot_data=False):\n",
    "        # train model itself\n",
    "        # instantiate variables\n",
    "        T_H0_epoch_num = {j: [] for j in range(num_epochs)}\n",
    "        T_epoch_num = {j: 0 for j in range(num_epochs)}\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=self.lr)\n",
    "        label_p = 1\n",
    "        label_q = -1\n",
    "        if self.model_type == 'logit':\n",
    "            label_p = 1\n",
    "            label_q = 0\n",
    "        # begin training\n",
    "        for epoch in range(num_epochs):\n",
    "            #clear_output(wait=True)\n",
    "            #print('Training epoch', epoch)\n",
    "            # set up information for plotting\n",
    "            feats_2_plot = []\n",
    "            predictions_2_plot = []\n",
    "            labels_2_plot = []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch in dset_test:\n",
    "                    feats = batch[:,:-1]\n",
    "                    label = batch[:,-1:]\n",
    "                    feats_2_plot.append(feats[:, :2])\n",
    "                    labels_2_plot.append(label)\n",
    "                    prediction, outputs, hadamard_outs = model(feats)\n",
    "                    predictions_2_plot.append(prediction)\n",
    "            # concatenate all items\n",
    "            plotting_feats = torch.cat(feats_2_plot, dim=0)\n",
    "            plotting_preds = torch.cat(predictions_2_plot, dim=0)\n",
    "            plotting_labels = torch.cat(labels_2_plot, dim=0)\n",
    "            \n",
    "            if plot_data: # plot the data\n",
    "                plt.scatter(plotting_feats.numpy()[:, 0],\n",
    "                            plotting_feats.numpy()[:, 1],\n",
    "                            c=plotting_preds.numpy().squeeze())\n",
    "                plt.colorbar()\n",
    "                plt.title('Actual Predictions of Model')\n",
    "                plt.show()\n",
    "        \n",
    "                plt.scatter(plotting_feats.numpy()[:, 0],\n",
    "                            plotting_feats.numpy()[:, 1],\n",
    "                            c=plotting_labels.numpy())\n",
    "                plt.colorbar()\n",
    "                plt.title('Labels for Dataset')\n",
    "                plt.show()\n",
    "        \n",
    "            T_1 = plotting_preds[plotting_labels == label_p].mean()\n",
    "            T_0 = plotting_preds[plotting_labels == label_q].mean()\n",
    "            T = T_1 - T_0\n",
    "        \n",
    "            T_H0 = []\n",
    "            for _ in range(m_perm):\n",
    "                dummy_labels = plotting_labels[torch.randperm(len(plotting_labels))]\n",
    "                T_1_dummy = plotting_preds[dummy_labels == label_p].mean()\n",
    "                T_0_dummy = plotting_preds[dummy_labels == label_q].mean()\n",
    "                T_H0.append(T_1_dummy - T_0_dummy)\n",
    "        \n",
    "            T_H0_epoch_num[epoch].extend(T_H0)\n",
    "            T_epoch_num[epoch] += T.item()\n",
    "            \n",
    "            model.train()\n",
    "            for batch in dset_train:\n",
    "                optimizer.zero_grad()\n",
    "                feats = batch[:,:-1]\n",
    "                label = batch[:,-1:]\n",
    "                batch_sz = feats.shape[0]\n",
    "                prediction, outputs, hadamard_outs = model(feats)\n",
    "                loss = self.loss_fcn(prediction, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            \n",
    "        return T_epoch_num, T_H0_epoch_num\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams for series of tests\n",
    "num_epochs = 15\n",
    "L = 2\n",
    "activation='relu'\n",
    "lr = 0.1\n",
    "num_tests = 100\n",
    "m_perm = 100\n",
    "d = 20\n",
    "trainN = 6000 \n",
    "testN = 1000\n",
    "batch_sz = 50\n",
    "model_type = 'logit'\n",
    "hidden_dim = 2*d\n",
    "plot_data=False\n",
    "thresh = 95\n",
    "ratios = [0.001, 0.0025, 0.005, 0.0075, 0.01, 0.1, 0.25, 0.5, 0.75, 1.0, 1.5, 2.0]\n",
    "lin_power_dict_1 = {ratio:{} for ratio in ratios}\n",
    "lin_power_dict_2 = {ratio:{} for ratio in ratios}\n",
    "log_power_dict_1 = {ratio:{} for ratio in ratios}\n",
    "log_power_dict_2 = {ratio:{} for ratio in ratios}\n",
    "ntk = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make saving data to fill code\n",
    "# consider parameters/points as 0.1, 0.5, 0.75, 1.0, 1.25, 1.5\n",
    "# 2 layers\n",
    "# get width vs time\n",
    "# epoch\n",
    "# plot statistical power as a function of time and width\n",
    "# for logistic, l2, ntk\n",
    "# take permutation test, find the threshold where 95% of the permutations are below the threshold\n",
    "# given this threshold, which fraction of the number of tests are above the threshold\n",
    "# do for each permutation test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run tests with different parameters and save powers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ratio in ratios:\n",
    "    for model_type in ['linear']:#, 'logit']:\n",
    "        trainer = Trainer(hidden_dim=hidden_dim, L = L, activation=activation,\n",
    "                          model_type=model_type, lr = lr, trainN=trainN, testN=testN, \n",
    "                          d=d, batch_sz=batch_sz, thresh=thresh, ratio = ratio)\n",
    "        mod1_power, mod2_power = trainer.train_and_plot(num_epochs, num_tests, m_perm=m_perm, \n",
    "                                                n_bins=50, plot_data=plot_data, ntk=ntk)\n",
    "        for epoch in mod1_power.keys(): mod1_power[epoch] = np.sum(mod1_power[epoch])/len(mod1_power[epoch])\n",
    "        for epoch in mod2_power.keys(): mod2_power[epoch] = np.sum(mod2_power[epoch])/len(mod2_power[epoch])\n",
    "        if model_type == 'linear':\n",
    "            lin_power_dict_1[ratio] = mod1_power\n",
    "            lin_power_dict_2[ratio] = mod2_power\n",
    "            lin_df_1 = pd.DataFrame(lin_power_dict_1)\n",
    "            lin_df_2 = pd.DataFrame(lin_power_dict_2)\n",
    "            lin_df_1.to_csv('lin_df_1.csv')\n",
    "            lin_df_2.to_csv('lin_df_2.csv')\n",
    "        else:\n",
    "            log_power_dict_1[ratio] = mod1_power\n",
    "            log_power_dict_2[ratio] = mod2_power\n",
    "            log_df_1 = pd.DataFrame(log_power_dict_1)\n",
    "            log_df_2 = pd.DataFrame(log_power_dict_2)\n",
    "            log_df_1.to_csv('log_df_1.csv')\n",
    "            log_df_2.to_csv('log_df_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot statistical power as function of time and neural network complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear output gaussian mixture model\n",
    "df = pd.DataFrame(lin_power_dict_1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(df, cmap='viridis', aspect='auto')\n",
    "cbar = plt.colorbar()  # Show colors\n",
    "cbar.set_label('Statistical Power')\n",
    "# Setting tick labels\n",
    "_, _ = plt.xticks(range(len(df.columns)), df.columns)\n",
    "_, _ = plt.yticks(range(len(df.index)), df.index)\n",
    "plt.title('Mixture Model')\n",
    "plt.xlabel('Ratio of Parameters to Number of Training Samples')\n",
    "plt.ylabel('Number of Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(lin_power_dict_2)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(df, cmap='viridis', aspect='auto')\n",
    "cbar = plt.colorbar()  # Show colors\n",
    "cbar.set_label('Statistical Power')\n",
    "# Setting tick labels\n",
    "_, _ = plt.xticks(range(len(df.columns)), df.columns)\n",
    "_, _ = plt.yticks(range(len(df.index)), df.index)\n",
    "plt.title('Gaussian Model')\n",
    "plt.xlabel('Ratio of Parameters to Number of Training Samples')\n",
    "plt.ylabel('Number of Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot logistic last layer statistical power as function of time and neural network complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear output gaussian mixture model\n",
    "df = pd.DataFrame(log_power_dict_1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(df, cmap='viridis', aspect='auto')\n",
    "cbar = plt.colorbar()  # Show colors\n",
    "cbar.set_label('Statistical Power | Logistic')\n",
    "# Setting tick labels\n",
    "_, _ = plt.xticks(range(len(df.columns)), df.columns)\n",
    "_, _ = plt.yticks(range(len(df.index)), df.index)\n",
    "plt.title('Mixture Model')\n",
    "plt.xlabel('Ratio of Parameters to Number of Training Samples')\n",
    "plt.ylabel('Number of Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(log_power_dict_2)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(df, cmap='viridis', aspect='auto')\n",
    "cbar = plt.colorbar()  # Show colors\n",
    "cbar.set_label('Statistical Power')\n",
    "# Setting tick labels\n",
    "_, _ = plt.xticks(range(len(df.columns)), df.columns)\n",
    "_, _ = plt.yticks(range(len(df.index)), df.index)\n",
    "plt.title('Gaussian Model | Logistic')\n",
    "plt.xlabel('Ratio of Parameters to Number of Training Samples')\n",
    "plt.ylabel('Number of Epochs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
